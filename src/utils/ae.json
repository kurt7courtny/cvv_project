{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "VideoInput",
            "id": "VideoInput-fmR93",
            "name": "video_out",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "video_in",
            "id": "AudioExtractor-tS410",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-VideoInput-fmR93{œdataTypeœ:œVideoInputœ,œidœ:œVideoInput-fmR93œ,œnameœ:œvideo_outœ,œoutput_typesœ:[œDataœ]}-AudioExtractor-tS410{œfieldNameœ:œvideo_inœ,œidœ:œAudioExtractor-tS410œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "VideoInput-fmR93",
        "sourceHandle": "{œdataTypeœ:œVideoInputœ,œidœ:œVideoInput-fmR93œ,œnameœ:œvideo_outœ,œoutput_typesœ:[œDataœ]}",
        "target": "AudioExtractor-tS410",
        "targetHandle": "{œfieldNameœ:œvideo_inœ,œidœ:œAudioExtractor-tS410œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "AudioExtractor",
            "id": "AudioExtractor-tS410",
            "name": "audio_out",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "audio_in",
            "id": "SpeakerDiarization-8ZuHH",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-AudioExtractor-tS410{œdataTypeœ:œAudioExtractorœ,œidœ:œAudioExtractor-tS410œ,œnameœ:œaudio_outœ,œoutput_typesœ:[œDataœ]}-SpeakerDiarization-8ZuHH{œfieldNameœ:œaudio_inœ,œidœ:œSpeakerDiarization-8ZuHHœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "AudioExtractor-tS410",
        "sourceHandle": "{œdataTypeœ:œAudioExtractorœ,œidœ:œAudioExtractor-tS410œ,œnameœ:œaudio_outœ,œoutput_typesœ:[œDataœ]}",
        "target": "SpeakerDiarization-8ZuHH",
        "targetHandle": "{œfieldNameœ:œaudio_inœ,œidœ:œSpeakerDiarization-8ZuHHœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "SpeakerDiarization",
            "id": "SpeakerDiarization-8ZuHH",
            "name": "result",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "seg_in",
            "id": "SegmentASR-y4Ysc",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-SpeakerDiarization-8ZuHH{œdataTypeœ:œSpeakerDiarizationœ,œidœ:œSpeakerDiarization-8ZuHHœ,œnameœ:œresultœ,œoutput_typesœ:[œDataœ]}-SegmentASR-y4Ysc{œfieldNameœ:œseg_inœ,œidœ:œSegmentASR-y4Yscœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "SpeakerDiarization-8ZuHH",
        "sourceHandle": "{œdataTypeœ:œSpeakerDiarizationœ,œidœ:œSpeakerDiarization-8ZuHHœ,œnameœ:œresultœ,œoutput_typesœ:[œDataœ]}",
        "target": "SegmentASR-y4Ysc",
        "targetHandle": "{œfieldNameœ:œseg_inœ,œidœ:œSegmentASR-y4Yscœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "SegmentASR",
            "id": "SegmentASR-y4Ysc",
            "name": "asr_out",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "analysis_in",
            "id": "AudioProcessor-AesxP",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-SegmentASR-y4Ysc{œdataTypeœ:œSegmentASRœ,œidœ:œSegmentASR-y4Yscœ,œnameœ:œasr_outœ,œoutput_typesœ:[œDataœ]}-AudioProcessor-AesxP{œfieldNameœ:œanalysis_inœ,œidœ:œAudioProcessor-AesxPœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "SegmentASR-y4Ysc",
        "sourceHandle": "{œdataTypeœ:œSegmentASRœ,œidœ:œSegmentASR-y4Yscœ,œnameœ:œasr_outœ,œoutput_typesœ:[œDataœ]}",
        "target": "AudioProcessor-AesxP",
        "targetHandle": "{œfieldNameœ:œanalysis_inœ,œidœ:œAudioProcessor-AesxPœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "id": "VideoInput-fmR93",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Select a local video file",
            "display_name": "1️⃣ Video Input",
            "documentation": "",
            "edited": true,
            "field_order": [
              "video_path"
            ],
            "frozen": false,
            "icon": "upload",
            "legacy": false,
            "lf_version": "1.4.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Video Path",
                "hidden": false,
                "method": "send",
                "name": "video_out",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from pathlib import Path\r\n\r\nfrom langflow.custom import Component\r\nfrom langflow.io import FileInput, Output\r\nfrom langflow.schema import Data\r\n\r\n\r\nclass VideoInput(Component):\r\n    display_name = \"1️⃣ Video Input\"\r\n    description = \"Select a local video file\"\r\n    icon = \"upload\"\r\n    name = \"VideoInput\"\r\n\r\n    inputs = [\r\n        FileInput(name=\"video_path\", display_name=\"Video File\", file_types=[\"mp4\", \"mov\", \"mkv\"], required=True),\r\n    ]\r\n    outputs = [\r\n        Output(name=\"video_out\", display_name=\"Video Path\", method=\"send\"),\r\n    ]\r\n\r\n    def send(self) -> Data:\r\n        path = Path(self.video_path)\r\n        if not path.exists():\r\n            raise FileNotFoundError(path)\r\n        self.status = f\"Loaded {path.name}\"\r\n        return Data(data={\"video_path\": str(path)})"
              },
              "video_path": {
                "_input_type": "FileInput",
                "advanced": false,
                "display_name": "Video File",
                "dynamic": false,
                "fileTypes": [
                  "mp4",
                  "mov",
                  "mkv"
                ],
                "file_path": "77cb8339-5fba-4e28-8178-b64a5c9bc012/22f7bac0-f188-456e-9970-ffe37fa76cfc.mp4",
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "video_path",
                "placeholder": "",
                "required": true,
                "show": true,
                "temp_file": false,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "VideoInput"
        },
        "dragging": false,
        "id": "VideoInput-fmR93",
        "measured": {
          "height": 236,
          "width": 320
        },
        "position": {
          "x": -163.09859286093172,
          "y": 17.4241083986306
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "AudioExtractor-tS410",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Separate audio track using FFmpeg",
            "display_name": "2️⃣ Audio Extractor",
            "documentation": "",
            "edited": true,
            "field_order": [
              "video_in",
              "duration"
            ],
            "frozen": false,
            "icon": "music",
            "legacy": false,
            "lf_version": "1.4.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Audio Path",
                "hidden": false,
                "method": "extract",
                "name": "audio_out",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from pathlib import Path\r\n\r\nfrom langflow.custom import Component\r\n# Import FloatInput to receive floating-point numbers as input\r\nfrom langflow.io import DataInput, FloatInput, Output\r\nfrom langflow.schema import Data\r\n\r\nfrom utils.cvv_utils import run_ffmpeg\r\n\r\n\r\nclass AudioExtractor(Component):\r\n    display_name = \"2️⃣ Audio Extractor\"\r\n    description = \"Separate audio track using FFmpeg\"\r\n    icon = \"music\"\r\n    name = \"AudioExtractor\"\r\n\r\n    inputs = [\r\n        DataInput(name=\"video_in\", display_name=\"Video Path\"),\r\n        # Add a floating-point input to control the audio extraction duration\r\n        StrInput(\r\n            name=\"duration\",\r\n            display_name=\"Extraction Duration (seconds)\",\r\n            info=\"Set the duration for extracting audio in seconds. If set to 0 or a negative value, the full audio will be extracted.\",\r\n            value=10,  # Default value is 0, which means extracting the full audio\r\n        ),\r\n    ]\r\n    outputs = [Output(name=\"audio_out\", display_name=\"Audio Path\", method=\"extract\")]\r\n\r\n    def extract(self) -> Data:\r\n        \"\"\"\r\n        Extract the audio track from a video file and trim it based on the user-defined duration.\r\n        \"\"\"\r\n        video = Path(self.video_in.data[\"video_path\"])\r\n        audio = video.with_suffix(\".wav\")\r\n\r\n        # Basic FFmpeg command\r\n        cmd = [\r\n            \"ffmpeg\",\r\n            \"-y\",          # Overwrite output file without asking\r\n            \"-i\", str(video), # Input file\r\n            \"-vn\",         # Remove video stream\r\n            \"-ac\", \"1\",      # Set audio channels to mono\r\n            \"-ar\", \"16000\",  # Set sampling rate to 16000 Hz\r\n        ]\r\n\r\n        cmd.extend([\"-t\", str(self.duration)])\r\n\r\n        # Add the output file path to the end of the command\r\n        cmd.append(str(audio))\r\n\r\n        # Execute the FFmpeg command\r\n        run_ffmpeg(cmd)\r\n\r\n        self.status = f\"Audio → {audio.name}\"\r\n        return Data(data={\"audio_path\": str(audio)})"
              },
              "duration": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Extraction Duration (seconds)",
                "dynamic": false,
                "info": "Set the duration for extracting audio in seconds. If set to 0 or a negative value, the full audio will be extracted.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "duration",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "300"
              },
              "video_in": {
                "_input_type": "DataInput",
                "advanced": false,
                "display_name": "Video Path",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "video_in",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "AudioExtractor"
        },
        "dragging": false,
        "id": "AudioExtractor-tS410",
        "measured": {
          "height": 274,
          "width": 320
        },
        "position": {
          "x": 239.8786048413947,
          "y": 15.734050601600686
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "SpeakerDiarization-8ZuHH",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "HuggingFace pyannote",
            "display_name": "3️⃣ Speaker Diarization",
            "documentation": "",
            "edited": true,
            "field_order": [
              "audio_in",
              "hf_token"
            ],
            "frozen": false,
            "icon": "users",
            "legacy": false,
            "lf_version": "1.4.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Speaker Diarization Result",
                "hidden": null,
                "method": "diarize",
                "name": "result",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "audio_in": {
                "_input_type": "DataInput",
                "advanced": false,
                "display_name": "Audio Path",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "audio_in",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from pathlib import Path\r\nfrom typing import List, Dict, Any\r\n\r\nfrom langflow.custom import Component\r\nfrom langflow.io import DataInput, StrInput, Output\r\nfrom langflow.schema import Data\r\nimport numpy as np\r\nimport torch\r\n\r\n\r\nclass SpeakerDiarization(Component):\r\n    display_name = \"3️⃣ Speaker Diarization\"\r\n    description = \"HuggingFace pyannote\"\r\n    icon = \"users\"\r\n    name = \"SpeakerDiarization\"\r\n\r\n    inputs = [\r\n        DataInput(name=\"audio_in\", display_name=\"Audio Path\"),\r\n        StrInput(\r\n            name=\"hf_token\",\r\n            display_name=\"HF Token (for private models)\",\r\n            advanced=True,\r\n            value=\"\",\r\n        ),\r\n    ]\r\n    outputs = [\r\n        Output(name=\"result\", display_name=\"Speaker Diarization Result\", method=\"diarize\"),\r\n    ]\r\n\r\n    def diarize(self) -> Data:\r\n        from pyannote.audio import Pipeline\r\n        from pyannote.audio import Model, Inference\r\n        from pyannote.core import Segment\r\n        import torch\r\n    \r\n        # Device setup\r\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    \r\n        # Load audio and models\r\n        audio_path = Path(self.audio_in.data[\"audio_path\"])\r\n        pipeline = Pipeline.from_pretrained(\r\n            \"pyannote/speaker-diarization-3.1\",\r\n            use_auth_token=self.hf_token or None,\r\n        ).to(device)\r\n    \r\n        embedding_model = Model.from_pretrained(\r\n            \"pyannote/embedding\",\r\n            use_auth_token=self.hf_token or None,\r\n        ).to(device)\r\n    \r\n        # Run diarization\r\n        diar = pipeline(str(audio_path))\r\n    \r\n        segs = []\r\n        \r\n        # Initialize inference for embeddings\r\n        embedding_inference = Inference(embedding_model, window=\"whole\")  # Set window size\r\n    \r\n        # Extract embeddings for each segment\r\n        for turn, _, speaker in diar.itertracks(yield_label=True):\r\n            segment = Segment(start=turn.start, end=turn.end)\r\n            try:\r\n                embedding = embedding_inference.crop(str(audio_path), segment)\r\n                # Check if embedding contains NaN values\r\n                if np.isnan(embedding).any():\r\n                    self.log(f\"NaN values detected in embedding for segment: {segment}. Skipping...\")\r\n                    continue\r\n            except Exception as e:\r\n                print(f\"Skipping segment {segment}: {str(e)}\")\r\n                continue\r\n            segs.append(\r\n                dict(\r\n                    speaker=speaker,\r\n                    start=round(turn.start, 3),\r\n                    end=round(turn.end, 3),\r\n                    embedding = embedding\r\n                )\r\n            )\r\n    \r\n        self.status = f\"Number of segments: {len(segs)}\"\r\n        return Data(\r\n            data={\r\n                \"segments\": segs,  # Only unmerged segments\r\n                \"audio_path\": str(audio_path),\r\n            }\r\n        )"
              },
              "hf_token": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "HF Token (for private models)",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "hf_token",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "SpeakerDiarization"
        },
        "dragging": false,
        "id": "SpeakerDiarization-8ZuHH",
        "measured": {
          "height": 192,
          "width": 320
        },
        "position": {
          "x": 647.1905690148157,
          "y": 11.937431939874614
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "SegmentASR-y4Ysc",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Whisper (HF pipeline)",
            "display_name": "4️⃣ Speech Recognition",
            "documentation": "",
            "edited": true,
            "field_order": [
              "seg_in",
              "num_speakers"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.4.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Transcription Result",
                "hidden": null,
                "method": "transcribe",
                "name": "asr_out",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from pathlib import Path\r\nfrom typing import List, Dict, Any\r\n\r\nimport torch\r\nfrom langflow.custom import Component\r\nfrom langflow.io import DataInput, Output\r\nfrom langflow.schema import Data\r\n\r\n# Import any utility functions like run_ffmpeg if needed\r\nfrom utils.cvv_utils import run_ffmpeg\r\n\r\nfrom itertools import groupby\r\nfrom operator import itemgetter\r\nimport whisper\r\n\r\nfrom sklearn.cluster import AgglomerativeClustering\r\nfrom sklearn.metrics.pairwise import cosine_distances\r\nimport numpy as np\r\n\r\n\r\nclass SegmentASR(Component):\r\n    display_name = \"4️⃣ Speech Recognition\"\r\n    description = \"Whisper (HF pipeline)\"\r\n    icon = \"type\"\r\n    name = \"SegmentASR\"\r\n\r\n    inputs = [\r\n        DataInput(name=\"seg_in\", display_name=\"Speaker Segments\"),\r\n        StrInput(\r\n            name=\"num_speakers\",\r\n            display_name=\"num of speakers\",\r\n            value=\"2\",\r\n        ),\r\n    ]\r\n    outputs = [Output(name=\"asr_out\", display_name=\"Transcription Result\", method=\"transcribe\")]\r\n\r\n    def transcribe(self) -> Data:\r\n        # Load the Whisper model\r\n        model = whisper.load_model(\"turbo\")\r\n\r\n        segs: List[Dict[str, Any]] = self.seg_in.data[\"segments\"]\r\n        audio_path = Path(self.seg_in.data[\"audio_path\"])\r\n        num_speakers = int(self.num_speakers)  # Retrieve the number of speakers from input\r\n\r\n        # Merge overlapping or adjacent segments for the same speaker\r\n        merged_segments = self.merge_segments_by_speaker(segs, num_speakers)\r\n\r\n        results = []\r\n        for seg in merged_segments:\r\n            # Ensure minimum duration for segments\r\n            MIN_SEGMENT_DURATION = 1.0  # in seconds\r\n            if seg[\"end\"] - seg[\"start\"] < MIN_SEGMENT_DURATION:\r\n                seg[\"end\"] = seg[\"start\"] + MIN_SEGMENT_DURATION\r\n\r\n            # Extract individual audio segments using FFmpeg\r\n            wav_seg = audio_path.with_name(\r\n                f\"{audio_path.stem}_{seg['speaker']}_{seg['start']:.2f}_{seg['end']:.2f}.wav\"\r\n            )\r\n            run_ffmpeg(\r\n                [\r\n                    \"ffmpeg\",\r\n                    \"-y\",\r\n                    \"-i\",\r\n                    str(audio_path),\r\n                    \"-ss\",\r\n                    str(seg[\"start\"]),\r\n                    \"-to\",\r\n                    str(seg[\"end\"]),\r\n                    str(wav_seg),\r\n                ]\r\n            )\r\n\r\n            # Use Whisper for transcription and language detection\r\n            result = model.transcribe(str(wav_seg))\r\n            text = result[\"text\"].strip()\r\n            language = result.get(\"language\", \"unknown\")  # Retrieve detected language\r\n\r\n            # Append result for this segment\r\n            results.append(\r\n                {**seg, \"text\": text, \"language\": language, \"wav\": str(wav_seg)}\r\n            )\r\n\r\n        # Update the component's status and return results\r\n        self.status = f\"ASR completed for {len(results)} segments\"\r\n        return Data(data={\"results\": results, \"audio_path\": str(audio_path)})\r\n\r\n    def merge_segments_by_speaker(\r\n        self, segments: List[Dict[str, Any]], num_speakers: int\r\n    ) -> List[Dict[str, Any]]:\r\n        \"\"\"\r\n        Merge overlapping or adjacent segments for the same speaker, clustering speakers using cosine distance of embeddings.\r\n        \"\"\"\r\n        if not segments:\r\n            return []\r\n\r\n        # Extract embeddings and segment information\r\n        embeddings = []\r\n        segment_data = []\r\n\r\n        for seg in segments:\r\n            # Assuming each segment has a precomputed \"embedding\" field\r\n            embedding = np.array(seg.get(\"embedding\", []))\r\n            if embedding.size > 0:\r\n                embeddings.append(embedding)\r\n                segment_data.append(seg)\r\n\r\n        if not embeddings:\r\n            raise ValueError(\"No embeddings found in the segment data.\")\r\n\r\n        # Perform clustering on the embeddings\r\n        embeddings = np.array(embeddings)\r\n        clustering = AgglomerativeClustering(\r\n            n_clusters=num_speakers, metric=\"cosine\", linkage=\"average\"\r\n        )\r\n        labels = clustering.fit_predict(embeddings)\r\n\r\n        # Assign speaker labels based on clustering\r\n        for idx, seg in enumerate(segment_data):\r\n            seg[\"speaker\"] = f\"Speaker_{labels[idx]}\"\r\n\r\n        # Group segments by speaker and merge overlapping or adjacent segments\r\n        segments_by_speaker = {}\r\n        for seg in segment_data:\r\n            speaker = seg[\"speaker\"]\r\n            if speaker not in segments_by_speaker:\r\n                segments_by_speaker[speaker] = []\r\n            segments_by_speaker[speaker].append(seg)\r\n\r\n        merged_segments = []\r\n        for speaker, segs in segments_by_speaker.items():\r\n            # Sort segments by start time\r\n            segs = sorted(segs, key=lambda x: x[\"start\"])\r\n\r\n            # Merge overlapping or adjacent segments\r\n            current_start = segs[0][\"start\"]\r\n            current_end = segs[0][\"end\"]\r\n            for seg in segs[1:]:\r\n                if seg[\"start\"] <= current_end:  # Overlapping or adjacent\r\n                    current_end = max(current_end, seg[\"end\"])  # Extend the current segment\r\n                else:\r\n                    # Add the merged segment for the current speaker\r\n                    merged_segments.append(\r\n                        {\"start\": current_start, \"end\": current_end, \"speaker\": speaker}\r\n                    )\r\n                    # Start a new segment\r\n                    current_start = seg[\"start\"]\r\n                    current_end = seg[\"end\"]\r\n\r\n            # Add the last segment for this speaker\r\n            merged_segments.append(\r\n                {\"start\": current_start, \"end\": current_end, \"speaker\": speaker}\r\n            )\r\n\r\n        return sorted(merged_segments, key=lambda x: x[\"start\"])"
              },
              "num_speakers": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "num of speakers",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "num_speakers",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "3"
              },
              "seg_in": {
                "_input_type": "DataInput",
                "advanced": false,
                "display_name": "Speaker Segments",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "seg_in",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "SegmentASR"
        },
        "dragging": false,
        "id": "SegmentASR-y4Ysc",
        "measured": {
          "height": 274,
          "width": 320
        },
        "position": {
          "x": 1068.5411539270146,
          "y": 14.870981460552173
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "AudioProcessor-AesxP",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Use as base for chat components.",
            "display_name": "Chat Component",
            "documentation": "",
            "edited": true,
            "field_order": [
              "analysis_in",
              "poe_api_key",
              "bot_name",
              "min_samplevoice_length"
            ],
            "frozen": false,
            "legacy": false,
            "lf_version": "1.4.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Audio and Text Preview",
                "hidden": null,
                "method": "generate_preview",
                "name": "preview_out",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "analysis_in": {
                "_input_type": "DataInput",
                "advanced": false,
                "display_name": "ASR Transcription Results",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "analysis_in",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "bot_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Poe Bot Name",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "bot_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "GPT-4"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import sqlite3\r\nfrom pathlib import Path\r\nfrom typing import List, Dict, Any\r\nimport json, base64\r\n\r\nfrom utils.cvv_utils import run_ffmpeg\r\n\r\nfrom langflow.base.io.chat import ChatComponent\r\nfrom langflow.io import DataInput, StrInput, Output\r\nfrom langflow.schema.properties import Source\r\nfrom langflow.schema.message import Message\r\n\r\n\r\nclass AudioProcessor(ChatComponent):\r\n    # Define component inputs\r\n    inputs = [\r\n        DataInput(name=\"analysis_in\", display_name=\"ASR Transcription Results\"),\r\n        StrInput(\r\n            name=\"poe_api_key\",\r\n            display_name=\"Poe API Key\",\r\n            advanced=True,\r\n            value=\"\"  # It's recommended to use a secure way to manage secrets\r\n        ),\r\n        StrInput(\r\n            name=\"bot_name\",\r\n            display_name=\"Poe Bot Name\",\r\n            value=\"GPT-4\"\r\n        ),\r\n        StrInput(\r\n            name=\"min_samplevoice_length\",\r\n            display_name=\"Min Sample Voice Length(s) For Clone\",\r\n            value=\"10\",\r\n        ),\r\n    ]\r\n    \r\n    # Define component outputs\r\n    outputs = [\r\n        Output(name=\"preview_out\", display_name=\"Audio and Text Preview\", method=\"generate_preview\")\r\n    ]\r\n    \r\n    async def _call_gpt4(self, prompt: str, api_key: str, bot_name: str) -> str:\r\n        \"\"\"\r\n        A helper function that wraps the synchronous call to fastapi_poe.\r\n        \"\"\"\r\n        import fastapi_poe as fp\r\n        message = fp.ProtocolMessage(role=\"user\", content=prompt)\r\n        response_text = \"\"\r\n        try:\r\n            async for partial_response in fp.get_bot_response(\r\n                messages=[message], bot_name=bot_name, api_key=api_key\r\n            ):\r\n                response_text += partial_response.text\r\n        except Exception as e:\r\n            self.log(f\"Error calling Poe API: {e}\")\r\n            return \"\"\r\n        return response_text\r\n\r\n    def _setup_database(self, db_path: str):\r\n        \"\"\"Initialize SQLite database and tables.\"\"\"\r\n        conn = sqlite3.connect(db_path)\r\n        cursor = conn.cursor()\r\n        cursor.execute('''\r\n            CREATE TABLE IF NOT EXISTS audio_segments (\r\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n                audio_name TEXT NOT NULL,\r\n                speaker_id TEXT NOT NULL,\r\n                text TEXT NOT NULL,\r\n                audio_binary BLOB NOT NULL,\r\n                duration REAL\r\n            )\r\n        ''')\r\n        conn.commit()\r\n        conn.close()\r\n\r\n    def _concat_and_split_audio(\r\n    self,\r\n    segments: List[Dict[str, Any]],\r\n) -> List[Dict[str, Any]]:\r\n        \"\"\"\r\n        Concatenate audio by speaker_id, then split into 10-second chunks.\r\n        \"\"\"\r\n        from pydub import AudioSegment\r\n        from io import BytesIO\r\n        import collections\r\n    \r\n        # 1. Group by speaker_id\r\n        group_map = collections.defaultdict(list)  # speaker_id: [segment_objs]\r\n        for seg in segments:\r\n            speaker_id = seg.get('speaker', 'unknown')\r\n            group_map[speaker_id].append(seg)\r\n    \r\n        split_segments = []\r\n    \r\n        # 2. Process each group\r\n        for speaker_id, seg_list in group_map.items():\r\n            concatenated_audio = AudioSegment.empty()\r\n            all_texts = []\r\n            duration_in_seconds = 0\r\n            # Sort segments by their start times\r\n            seg_list.sort(key=lambda x: x.get('start', 0))\r\n    \r\n            for seg in seg_list:\r\n                start_time = seg.get('start', 0)\r\n                end_time = seg.get('end', 0)\r\n                audio_file = seg.get('wav')\r\n    \r\n                try:\r\n                    # Extract and concatenate audio\r\n                    audio_segment = AudioSegment.from_file(audio_file)\r\n                    audio_portion = audio_segment[start_time * 1000:end_time * 1000]\r\n                    concatenated_audio += audio_portion\r\n                    duration_in_seconds += end_time - start_time\r\n                    all_texts.append(seg.get('text', ''))\r\n                except Exception as e:\r\n                    self.log(f\"Error processing audio segment: {e}\")\r\n    \r\n            # Split concatenated audio into 10-second chunks\r\n            self.log(f\"speaker segments length: {speaker_id}: {duration_in_seconds}\")\r\n            min_samplevoice_length = int(self.min_samplevoice_length)\r\n            for i in range(0, int(duration_in_seconds), min_samplevoice_length):\r\n                if i == min_samplevoice_length:\r\n                    split_audio = concatenated_audio[i * 1000:(i + min_samplevoice_length) * 1000]\r\n                    audio_binary = BytesIO()\r\n                    split_audio.export(audio_binary, format=\"wav\")\r\n        \r\n                    # Build metadata for this chunk\r\n                    text_summary = (\r\n                        f\"Speaker {speaker_id}, Part {i // min_samplevoice_length + 1}: \"\r\n                        + \" \".join(all_texts)\r\n                    )\r\n        \r\n                    split_segments.append({\r\n                        \"speaker_id\": speaker_id,\r\n                        \"audio_binary\": audio_binary.getvalue(),\r\n                        \"duration\": min(min_samplevoice_length, duration_in_seconds - i),\r\n                        \"text\": text_summary\r\n                    })\r\n    \r\n        return split_segments\r\n\r\n    async def process_and_store(self, db_path) -> Data:\r\n        \"\"\"\r\n        Core processing logic: Analyze, filter, concatenate, split, and store.\r\n        \"\"\"\r\n        # Retrieve data from input\r\n        segments_data: List[Dict[str, Any]] = self.analysis_in.data[\"results\"]\r\n    \r\n        if not segments_data:\r\n            self.log(f\"Input data is empty, nothing to process.\")\r\n            return Data(data={\"processed_count\": 0})\r\n    \r\n        # Initialize database\r\n        self._setup_database(db_path)\r\n    \r\n        audio_path_str = self.analysis_in.data.get(\"audio_path\")\r\n        audio_path = Path(audio_path_str)\r\n    \r\n        # --- Skip Step 1: Semantic Segmentation ---\r\n        # Directly use all sentences in the input data.\r\n    \r\n        # --- Step 2: Scene Relevance Filtering ---\r\n        self.log(f\"Step 2: Filtering irrelevant sentences...{segments_data}\")\r\n        all_indices_to_keep = set()\r\n    \r\n        # Treat all sentences as part of a single segment\r\n        segment_text = \"\\n\".join(\r\n            [f\"Sentence {i}: \\\"{seg['text']}\\\"\" for i, seg in enumerate(segments_data)]\r\n        )\r\n    \r\n        prompt2 = (\r\n            f\"You are a conversation editing expert. Below is a conversation segment with numbered sentences.\\n\"\r\n            f\"Your task is to identify and remove all irrelevant sentences (e.g., small talk or background noise).\\n\"\r\n            f\"Return the indices of sentences to keep in JSON format.\\n\"\r\n            f\"Example: {{'kept_sentences': [0, 1, 2]}}\\n\\n\"\r\n            f\"Conversation segment:\\n{segment_text}\"\r\n        )\r\n    \r\n        response2_str = await self._call_gpt4(prompt2, self.poe_api_key, self.bot_name)\r\n    \r\n        try:\r\n            if \"```json\" in response2_str:\r\n                response2_str = response2_str.split(\"```json\\n\")[1].split(\"```\")[0]\r\n            kept_info = json.loads(response2_str)\r\n            all_indices_to_keep.update(kept_info.get('kept_sentences', []))\r\n        except (json.JSONDecodeError, TypeError, IndexError):\r\n            self.log(f\"Warning: Parsing failed, keeping all sentences.\")\r\n            all_indices_to_keep.update(range(len(segments_data)))\r\n    \r\n        # Filter segments for next step\r\n        segments_for_step3 = [seg for i, seg in enumerate(segments_data) if i in all_indices_to_keep]\r\n    \r\n        # --- Step 3: Concatenate and Split Audio ---\r\n        self.log(f\"Step 3: Concatenating and splitting audio...{segments_for_step3}\")\r\n        split_audio_segments = self._concat_and_split_audio(segments_for_step3)\r\n    \r\n        # --- Step 4: Store in Database ---\r\n        self.log(f\"Step 4: Storing {len(split_audio_segments)} results in the database...\")\r\n        conn = sqlite3.connect(db_path)\r\n        cursor = conn.cursor()\r\n    \r\n        for res in split_audio_segments:\r\n            cursor.execute(\r\n                \"\"\"\r\n                INSERT INTO audio_segments (audio_name, speaker_id, text, audio_binary, duration)\r\n                VALUES (?, ?, ?, ?, ?)\r\n                \"\"\",\r\n                (\r\n                    audio_path.stem,\r\n                    res['speaker_id'],\r\n                    res['text'],\r\n                    res['audio_binary'],\r\n                    res['duration']\r\n                )\r\n            )\r\n    \r\n        conn.commit()\r\n        conn.close()\r\n    \r\n        self.log(f\"Processing complete! Stored {len(split_audio_segments)} audio segments in the database.\")\r\n\r\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\r\n        source_dict = {}\r\n        if id_:\r\n            source_dict[\"id\"] = id_\r\n        if display_name:\r\n            source_dict[\"display_name\"] = display_name\r\n        if source:\r\n            # Handle case where source is a ChatOpenAI object\r\n            if hasattr(source, \"model_name\"):\r\n                source_dict[\"source\"] = source.model_name\r\n            elif hasattr(source, \"model\"):\r\n                source_dict[\"source\"] = str(source.model)\r\n            else:\r\n                source_dict[\"source\"] = str(source)\r\n        return Source(**source_dict)\r\n        \r\n    async def generate_preview(self) -> Message:\r\n        \"\"\"\r\n        Generate an audio and text preview.\r\n        \"\"\"\r\n    \r\n        db_path = \"processed_audio.db\"\r\n    \r\n        # Ensure the database and table exist\r\n        self._setup_database(db_path)\r\n    \r\n        # Connect to the database and retrieve data\r\n        try:\r\n            conn = sqlite3.connect(db_path)\r\n            cursor = conn.cursor()\r\n    \r\n            # Process and store audio segments\r\n            await self.process_and_store(db_path)\r\n    \r\n            # Fetch text and audio binary from the database\r\n            cursor.execute(\"SELECT text, audio_binary FROM audio_segments\")\r\n            rows = cursor.fetchall()\r\n        except sqlite3.Error as e:\r\n            raise RuntimeError(f\"Database error: {e}\")\r\n        finally:\r\n            conn.close()\r\n    \r\n        # Build HTML preview\r\n        html = \"<div><h3>Audio and Text Preview</h3>\"\r\n        for text, audio_binary in rows:\r\n            audio_src = f\"data:audio/wav;base64,{base64.b64encode(audio_binary).decode()}\"\r\n            html += f\"\"\"\r\n            <div style=\"margin-bottom: 20px;\">\r\n                <p><strong>Text:</strong> {text}</p>\r\n                <audio controls>\r\n                    <source src=\"{audio_src}\" type=\"audio/wav\">\r\n                    Your browser does not support audio playback.\r\n                </audio>\r\n            </div>\r\n            \"\"\"\r\n        html += \"</div>\"\r\n    \r\n        # Get source properties\r\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\r\n        \r\n        # Fallback for missing attributes\r\n        background_color = getattr(self, \"background_color\", \"#FFFFFF\")  # Default white\r\n        text_color = getattr(self, \"text_color\", \"#000000\")  # Default black\r\n        if hasattr(self, \"chat_icon\"):\r\n            icon = self.chat_icon\r\n    \r\n        # Create a new Message\r\n        message = Message(text=html)\r\n    \r\n        # Set message properties\r\n        self.session_id = \"\"\r\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\r\n        message.properties.source = self._build_source(source_id, display_name, source)\r\n        message.properties.icon = icon\r\n        message.properties.background_color = background_color\r\n        message.properties.text_color = text_color\r\n    \r\n        # Store message if required\r\n        if self.session_id and self.should_store_message:\r\n            try:\r\n                stored_message = await self.send_message(message)\r\n                self.message.value = stored_message\r\n                message = stored_message\r\n            except Exception as e:\r\n                raise RuntimeError(f\"Failed to store message: {e}\")\r\n    \r\n        # Update status and return the message\r\n        self.status = message\r\n        return message"
              },
              "min_samplevoice_length": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Min Sample Voice Length(s) For Clone",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "min_samplevoice_length",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "10"
              },
              "poe_api_key": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "Poe API Key",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "poe_api_key",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "AudioProcessor"
        },
        "dragging": false,
        "id": "AudioProcessor-AesxP",
        "measured": {
          "height": 357,
          "width": 320
        },
        "position": {
          "x": -165.3053418985678,
          "y": 329.24696832809286
        },
        "selected": true,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": 140.74415032048518,
      "y": 62.093120941545465,
      "zoom": 0.5849998712498528
    }
  },
  "description": "audio extractor",
  "endpoint_name": null,
  "id": "040b3bfb-610d-4e7e-9ae0-238f971fb93a",
  "is_component": false,
  "last_tested_version": "1.4.3",
  "name": "ae",
  "tags": []
}